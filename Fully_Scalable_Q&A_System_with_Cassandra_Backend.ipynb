{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtreamgit/06_web/blob/master/Fully_Scalable_Q%26A_System_with_Cassandra_Backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka26TT2NOjFP"
      },
      "source": [
        "# Vector Similarity Search QA Quickstart\n",
        "\n",
        "Set up a simple Question-Answering system with LangChain and CassIO, using Cassandra as the Vector Database."
      ],
      "id": "ka26TT2NOjFP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ptVLMHD4thkQ"
      },
      "id": "ptVLMHD4thkQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu175kyyOjFS"
      },
      "source": [
        "## Colab-specific setup"
      ],
      "id": "wu175kyyOjFS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2jVhACeOjFT"
      },
      "source": [
        "Make sure you have a Database and get ready to upload the Secure Connect Bundle and supply the Token string\n",
        "(see [Pre-requisites](https://cassio.org/start_here/#vector-database) on cassio.org for details. Remember you need a **custom Token** with role [Database Administrator](https://awesome-astra.github.io/docs/pages/astra/create-token/)).\n",
        "\n",
        "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly\n",
        "(see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for details).\n",
        "\n",
        "_Note: this notebook is modified from the CassIO documentation. Visit [this page on cassIO.org](https://cassio.org/frameworks/langchain/qa-basic/)._\n"
      ],
      "id": "A2jVhACeOjFT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2953d95b",
      "metadata": {
        "id": "2953d95b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e65bed-91ad-42b8-e96f-ee580a63e1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langchain (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.12.0 which is incompatible.\n",
            "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.12.3 which is incompatible.\n",
            "tensorflow 2.14.0 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install required dependencies\n",
        "! pip install -q --progress-bar off \\\n",
        "    \"git+https://github.com/hemidactylus/langchain@updated-full-preview--lab#egg=langchain&subdirectory=libs/langchain\" \\\n",
        "    \"cassio>=0.1.1\" \\\n",
        "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
        "    \"jupyter>=1.0.0\" \\\n",
        "    \"openai==0.27.7\" \\\n",
        "    \"python-dotenv==1.0.0\" \\\n",
        "    \"tensorflow-cpu==2.12.0\" \\\n",
        "    \"tiktoken==0.4.0\" \\\n",
        "    \"transformers>=4.29.2\"\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222f44ff",
      "metadata": {
        "id": "222f44ff"
      },
      "source": [
        "⚠️ **Do not mind a \"Your session crashed...\" message you may see.**\n",
        "\n",
        "It was us, making sure your kernel restarts with all the correct dependency versions. _You can now proceed with the notebook._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgpMOv5ZOjFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775ee53e-25ce-4df4-b03e-66c1355735ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Astra DB Keyspace name (e.g. cassio_tutorials): pg_vsearch\n"
          ]
        }
      ],
      "source": [
        "# Input your database keyspace name:\n",
        "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name (e.g. cassio_tutorials): ')"
      ],
      "id": "lgpMOv5ZOjFW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIefcinlOjFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188cfbe4-1998-4fc7-d0bd-74e600284bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Astra DB Token (\"AstraCS:...\"): ··········\n"
          ]
        }
      ],
      "source": [
        "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
        "from getpass import getpass\n",
        "ASTRA_DB_APPLICATION_TOKEN = getpass('Your Astra DB Token (\"AstraCS:qtcgniCjcQbMYdCIhiRmzTgy:ddef6e99e58d223be927d5bbcea0ae5f2c043b4c5379f6e2780e4f15da5bfa67\"): ')"
      ],
      "id": "gIefcinlOjFW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNQ6T_Gjk0Oz"
      },
      "source": [
        "### Astra DB Secure Connect Bundle\n",
        "\n",
        "Please upload the Secure Connect Bundle zipfile to connect to your Astra DB instance.\n",
        "\n",
        "The Secure Connect Bundle is needed to establish a secure connection to the database.\n",
        "Click [here](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure) for instructions on how to download it from Astra DB."
      ],
      "id": "QNQ6T_Gjk0Oz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ds5ElT5OjFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "513b36bd-06c3-4558-a694-0c5b037647de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Secure Connect Bundle\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-60a96389-6915-41cd-8ceb-43748882a480\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-60a96389-6915-41cd-8ceb-43748882a480\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving secure-connect-db-paulgraham.zip to secure-connect-db-paulgraham.zip\n"
          ]
        }
      ],
      "source": [
        "# Upload your Secure Connect Bundle zipfile:\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "print('Please upload your Secure Connect Bundle')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
        "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
        "    )"
      ],
      "id": "8ds5ElT5OjFX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvET8h1JOjFY"
      },
      "outputs": [],
      "source": [
        "# colab-specific override of helper functions\n",
        "from cassandra.cluster import (\n",
        "    Cluster,\n",
        ")\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "\n",
        "def getCQLSession(mode='astra_db'):\n",
        "    if mode == 'astra_db':\n",
        "        cluster = Cluster(\n",
        "            cloud={\n",
        "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
        "            },\n",
        "            auth_provider=PlainTextAuthProvider(\n",
        "                \"token\",\n",
        "                ASTRA_DB_APPLICATION_TOKEN,\n",
        "            ),\n",
        "        )\n",
        "        astraSession = cluster.connect()\n",
        "        return astraSession\n",
        "    else:\n",
        "        raise ValueError('Unsupported CQL Session mode')\n",
        "\n",
        "def getCQLKeyspace(mode='astra_db'):\n",
        "    if mode == 'astra_db':\n",
        "        return ASTRA_DB_KEYSPACE\n",
        "    else:\n",
        "        raise ValueError('Unsupported CQL Session mode')"
      ],
      "id": "AvET8h1JOjFY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXCQ6T_Gjk0Oz"
      },
      "source": [
        "### LLM Provider\n",
        "\n",
        "In the cell below you can choose between **GCP Vertex AI** or **OpenAI** for your LLM services.\n",
        "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
        "\n",
        "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
      ],
      "id": "QXCQ6T_Gjk0Oz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUozIGd9OjFY"
      },
      "outputs": [],
      "source": [
        "# Set your secret(s) for LLM access:\n",
        "llmProvider = 'OpenAI'  # 'GCP_VertexAI', 'Azure_OpenAI'\n"
      ],
      "id": "JUozIGd9OjFY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp7x3DyKOjFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b67aae-db17-4f11-a429-211f86035a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your secret for LLM provider \"OpenAI\": ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "if llmProvider == 'OpenAI':\n",
        "    apiSecret = getpass(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
        "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
        "elif llmProvider == 'GCP_VertexAI':\n",
        "    # we need a json file\n",
        "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
        "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'No file uploaded. Please re-run the cell.'\n",
        "        )\n",
        "elif llmProvider == 'Azure_OpenAI':\n",
        "    # a few parameters must be input\n",
        "    apiSecret = input(f'Your API Key for LLM provider \"{llmProvider}\": ')\n",
        "    os.environ['AZURE_OPENAI_API_KEY'] = apiSecret\n",
        "    apiBase = input('The \"Base URL\" for your models (e.g. \"https://YOUR-RESOURCE-NAME.openai.azure.com\"): ')\n",
        "    os.environ['AZURE_OPENAI_API_BASE'] = apiBase\n",
        "    apiLLMDepl = input('The name of your LLM Deployment: ')\n",
        "    os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'] = apiLLMDepl\n",
        "    apiLLMModel = input('The name of your LLM Model (e.g. \"gpt-4\"): ')\n",
        "    os.environ['AZURE_OPENAI_LLM_MODEL'] = apiLLMModel\n",
        "    apiEmbDepl = input('The name for your Embeddings Deployment: ')\n",
        "    os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'] = apiEmbDepl\n",
        "    apiEmbModel = input('The name of your Embedding Model (e.g. \"text-embedding-ada-002\"): ')\n",
        "    os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'] = apiEmbModel\n",
        "\n",
        "    # The following is probably not going to change for some time...\n",
        "    os.environ['AZURE_OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
        "else:\n",
        "    raise ValueError('Unknown/unsupported LLM Provider')"
      ],
      "id": "dp7x3DyKOjFZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuqBtD_fOjFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3f0cea-671c-45e1-de4e-5a8984888f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 75047  100 75047    0     0   366k      0 --:--:-- --:--:-- --:--:--  366k\n"
          ]
        }
      ],
      "source": [
        "# retrieve the text of a few documents that will be indexed in the vector store\n",
        "! mkdir texts\n",
        "! curl https://raw.githubusercontent.com/jerryjliu/llama_index/main/examples/paul_graham_essay/data/paul_graham_essay.txt --output texts/paul_graham_essay.txt"
      ],
      "id": "JuqBtD_fOjFZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS_TO8RbOjFZ"
      },
      "source": [
        "### Colab preamble completed\n",
        "\n",
        "The following cells constitute the demo notebook proper."
      ],
      "id": "IS_TO8RbOjFZ"
    },
    {
      "cell_type": "markdown",
      "id": "6715bc2b",
      "metadata": {
        "id": "6715bc2b"
      },
      "source": [
        "# Vector Similarity Search QA Quickstart\n",
        "\n",
        "Set up a simple Question-Answering system with LangChain and CassIO, using Cassandra as the Vector Database."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761d9b70",
      "metadata": {
        "id": "761d9b70"
      },
      "source": [
        "_**NOTE:** this uses Cassandra's \"Vector Similarity Search\" capability.\n",
        "Make sure you are connecting to a vector-enabled database for this demo._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042f832e",
      "metadata": {
        "id": "042f832e"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4388ac1d",
      "metadata": {
        "id": "4388ac1d"
      },
      "source": [
        "The following line imports the Cassandra flavor of a LangChain vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65c46f0",
      "metadata": {
        "id": "d65c46f0"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4578a87b",
      "metadata": {
        "id": "4578a87b"
      },
      "source": [
        "A database connection is needed to access Cassandra. The following assumes\n",
        "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11013224",
      "metadata": {
        "id": "11013224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac7abee-0f63-46db-99e0-47f1c508a853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 8322abed-c007-43de-9e8a-5dddb00a380a-us-east1.db.astra.datastax.com:29042:4826ba60-d274-4b3b-944f-51d42fe5ae61. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 8322abed-c007-43de-9e8a-5dddb00a380a-us-east1.db.astra.datastax.com:29042:4826ba60-d274-4b3b-944f-51d42fe5ae61. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(138953527602832) 8322abed-c007-43de-9e8a-5dddb00a380a-us-east1.db.astra.datastax.com:29042:4826ba60-d274-4b3b-944f-51d42fe5ae61> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 8322abed-c007-43de-9e8a-5dddb00a380a-us-east1.db.astra.datastax.com:29042:4826ba60-d274-4b3b-944f-51d42fe5ae61. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "# creation of the DB connection\n",
        "cqlMode = 'astra_db'\n",
        "session = getCQLSession(mode=cqlMode)\n",
        "keyspace = getCQLKeyspace(mode=cqlMode)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e2a156",
      "metadata": {
        "id": "32e2a156"
      },
      "source": [
        "Both an LLM and an embedding function are required.\n",
        "\n",
        "Below is the logic to instantiate the LLM and embeddings of choice. We chose to leave it in the notebooks for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124e3de4",
      "metadata": {
        "id": "124e3de4",
        "outputId": "5a2bd15f-36ac-4b59-bebc-0b3aa02481ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM+embeddings from OpenAI\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# creation of the LLM resources\n",
        "\n",
        "\n",
        "if llmProvider == 'GCP_VertexAI':\n",
        "    from langchain.llms import VertexAI\n",
        "    from langchain.embeddings import VertexAIEmbeddings\n",
        "    llm = VertexAI()\n",
        "    myEmbedding = VertexAIEmbeddings()\n",
        "    print('LLM+embeddings from Vertex AI')\n",
        "elif llmProvider == 'OpenAI':\n",
        "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
        "    from langchain.llms import OpenAI\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    llm = OpenAI(temperature=0)\n",
        "    myEmbedding = OpenAIEmbeddings()\n",
        "    print('LLM+embeddings from OpenAI')\n",
        "elif llmProvider == 'Azure_OpenAI':\n",
        "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
        "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
        "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
        "    from langchain.llms import AzureOpenAI\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
        "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
        "    myEmbedding = OpenAIEmbeddings(model=os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'],\n",
        "                                   deployment=os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'])\n",
        "    print('LLM+embeddings from Azure OpenAI')\n",
        "else:\n",
        "    raise ValueError('Unknown LLM provider.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285f29cf",
      "metadata": {
        "id": "285f29cf"
      },
      "source": [
        "## A minimal example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf74a31",
      "metadata": {
        "id": "5cf74a31"
      },
      "source": [
        "The following is a minimal usage of the Cassandra vector store. The store is created and filled at once, and is then queried to retrieve relevant parts of the indexed text, which are then stuffed into a prompt finally used to answer a question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f29fc57",
      "metadata": {
        "id": "6f29fc57"
      },
      "source": [
        "The following creates an \"index creator\", which knows about the type of vector store, the embedding to use and how to preprocess the input text:\n",
        "\n",
        "_(Note: stores built with different embedding functions will need different tables. This is why we append the `llmProvider` name to the table name in the next cell.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cfe71b",
      "metadata": {
        "id": "d2cfe71b"
      },
      "outputs": [],
      "source": [
        "table_name = 'vs_test1_' + llmProvider\n",
        "\n",
        "index_creator = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=Cassandra,\n",
        "    embedding=myEmbedding,\n",
        "    text_splitter=CharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=0,\n",
        "    ),\n",
        "    vectorstore_kwargs={\n",
        "        'session': session,\n",
        "        'keyspace': keyspace,\n",
        "        'table_name': table_name,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5ddfb6",
      "metadata": {
        "id": "ea5ddfb6"
      },
      "source": [
        "Loading a local text ( Great work by http://paulgraham.com/greatwork.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8d65df",
      "metadata": {
        "id": "de8d65df"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader('texts/greatwork.txt', encoding='utf8')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('texts/paul_graham_essay.txt', encoding='utf8')"
      ],
      "metadata": {
        "id": "je7UARDCcQMW"
      },
      "id": "je7UARDCcQMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "53431eca",
      "metadata": {
        "id": "53431eca"
      },
      "source": [
        "This takes a few seconds to run, as it must calculate embedding vectors for a number of chunks of the input text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4929f2a",
      "metadata": {
        "id": "a4929f2a"
      },
      "outputs": [],
      "source": [
        "# Note: Certain LLM providers need workaround to evaluate batch embeddings\n",
        "#       (as done in next cell).\n",
        "#       As of 2023-06-29, Azure OpenAI would  error with:\n",
        "#           \"InvalidRequestError: Too many inputs. The max number of inputs is 1\"\n",
        "if llmProvider == 'Azure_OpenAI':\n",
        "    from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "    docs = loader.load()\n",
        "    subdocs = index_creator.text_splitter.split_documents(docs)\n",
        "    #\n",
        "    print(f'subdocument {0} ...', end=' ')\n",
        "    vs = index_creator.vectorstore_cls.from_documents(\n",
        "        subdocs[:1],\n",
        "        index_creator.embedding,\n",
        "        **index_creator.vectorstore_kwargs,\n",
        "    )\n",
        "    print('done.')\n",
        "    for sdi, sd in enumerate(subdocs[1:]):\n",
        "        print(f'subdocument {sdi+1} ...', end=' ')\n",
        "        vs.add_texts(texts=[sd.page_content], metadata=[sd.metadata])\n",
        "        print('done.')\n",
        "    #\n",
        "    index = VectorStoreIndexWrapper(vectorstore=vs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c9970e",
      "metadata": {
        "id": "64c9970e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547d5020-42cf-4372-d943-ab9e000ec005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 508, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 777, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 557, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 587, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 622, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 775, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 456, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 604, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 618, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 453, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 481, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 520, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 417, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 495, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 602, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1004, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1203, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 844, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 407, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 910, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 674, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 474, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 814, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 530, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 469, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 489, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 433, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 603, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 772, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 571, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 594, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 458, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 417, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 402, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 628, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 689, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 641, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 473, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 414, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 585, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 764, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 502, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 640, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 507, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 564, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 707, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 615, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 733, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 497, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 625, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 468, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 576, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 534, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 427, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 412, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 417, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 528, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 565, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 487, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 470, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 552, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 427, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 596, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 403, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1025, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 438, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 900, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 614, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 635, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 443, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 478, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 473, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 549, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 644, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 402, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 489, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 551, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 527, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 563, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 472, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 511, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 419, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 484, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 499, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 480, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 634, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 611, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 478, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 526, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 637, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 409, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 591, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 543, which is longer than the specified 400\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 758, which is longer than the specified 400\n"
          ]
        }
      ],
      "source": [
        "if llmProvider != 'Azure_OpenAI':\n",
        "    index = index_creator.from_loaders([loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c02057",
      "metadata": {
        "id": "50c02057"
      },
      "source": [
        "### Check what's on DB\n",
        "\n",
        "By way of demonstration, if you were to directly read the rows stored in your database table, this is what you would now find there (not that you'll ever _have to_, for LangChain and CassIO provide an abstraction on top of that):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de08db04",
      "metadata": {
        "id": "de08db04",
        "outputId": "63fd5d47-f855-456a-a1fd-7257f5e3c54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Row 0:\n",
            "    row_id:            a09800c9527a44f2aa97e5525a939a07\n",
            "    vector:            [-0.0021464480087161064, -0.010701625607907772, 0.00421805959194 ...\n",
            "    body_blob:         Over the next several years I wrote lots of essays about all kin ...\n",
            "    metadata_s:        {'source': 'texts/paul_graham_essay.txt'}\n",
            "\n",
            "Row 1:\n",
            "    row_id:            ab2b7b7ce93e4f5c92382d7f1d349953\n",
            "    vector:            [-0.01360295433551073, -0.01684478111565113, 0.02700249850749969 ...\n",
            "    body_blob:         It's not necessarily a bad sign if work is a struggle, any more  ...\n",
            "    metadata_s:        {'source': 'texts/greatwork.txt'}\n",
            "\n",
            "Row 2:\n",
            "    row_id:            780cb6069fd34c4faa363c99e4510001\n",
            "    vector:            [0.0032079373486340046, 0.01262128446251154, 0.00697820913046598 ...\n",
            "    body_blob:         [18] The principles defining a religion have to be mistaken. Oth ...\n",
            "    metadata_s:        {'source': 'texts/greatwork.txt'}\n",
            "\n",
            "Row 3:\n",
            "    row_id:            712357b9bac249828c893549475a3b01\n",
            "    vector:            [0.013521475717425346, -0.0009003786253742874, 0.013496201485395 ...\n",
            "    body_blob:         It's a great thing to be rich in unanswered questions. And this  ...\n",
            "    metadata_s:        {'source': 'texts/greatwork.txt'}\n",
            "\n",
            "Row 4:\n",
            "    row_id:            6f3b37f20bb142d5a21f46d73b6076b7\n",
            "    vector:            [0.012477803975343704, -0.0017558486433699727, 0.007757382933050 ...\n",
            "    body_blob:         Whereas some of the very best work will seem like it took compar ...\n",
            "    metadata_s:        {'source': 'texts/greatwork.txt'}\n",
            "\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "cqlSelect = f'SELECT * FROM {keyspace}.{table_name} LIMIT 5;'  # (Not a production-optimized query ...)\n",
        "rows = session.execute(cqlSelect)\n",
        "for row_i, row in enumerate(rows):\n",
        "    print(f'\\nRow {row_i}:')\n",
        "    # depending on the cassIO version, the underlying Cassandra table can have different structure ...\n",
        "    try:\n",
        "        # you are using the new cassIO 0.1.0+ : congratulations :)\n",
        "        print(f'    row_id:            {row.row_id}')\n",
        "        print(f'    vector:            {str(row.vector)[:64]} ...')\n",
        "        print(f'    body_blob:         {row.body_blob[:64]} ...')\n",
        "        print(f'    metadata_s:        {row.metadata_s}')\n",
        "    except AttributeError:\n",
        "        # Please upgrade your cassIO to the latest version ...\n",
        "        print(f'    document_id:      {row.document_id}')\n",
        "        print(f'    embedding_vector: {str(row.embedding_vector)[:64]} ...')\n",
        "        print(f'    document:         {row.document[:64]} ...')\n",
        "        print(f'    metadata_blob:    {row.metadata_blob}')\n",
        "\n",
        "print('\\n...')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d9f48b",
      "metadata": {
        "id": "29d9f48b"
      },
      "source": [
        "### Ask a question, get an answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f35aaa7",
      "metadata": {
        "id": "3f35aaa7",
        "outputId": "21c31593-c9a4-437a-c224-d508cb74fe97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Doing great work means doing something important so well that you expand people's ideas of what's possible. It needs to have three qualities: it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "query = \"What are the qualities of doing great work?\"\n",
        "index.query(query, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527a4697",
      "metadata": {
        "id": "527a4697"
      },
      "source": [
        "## Spawning a \"retriever\" from the index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb6b732",
      "metadata": {
        "id": "4cb6b732"
      },
      "source": [
        "You just saw how easily you can plug a Cassandra-backed Vector Index into a full question-answering LangChain pipeline.\n",
        "\n",
        "But you can as easily work at a slightly lower level: the following code spawns a `VectorStoreRetriever` from the index for manual [retrieval](https://python.langchain.com/en/latest/modules/indexes/retrievers.html) of documents related to a given query text. The results are instances of LangChain's `Document` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bae5520",
      "metadata": {
        "id": "7bae5520"
      },
      "outputs": [],
      "source": [
        "retriever = index.vectorstore.as_retriever(search_kwargs={\n",
        "    'k': 2,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e816ee",
      "metadata": {
        "id": "e4e816ee",
        "outputId": "cd5ef670-41fd-491f-be23-651d1c1cf236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren\\'t consciously aware of much we\\'re seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.', metadata={'source': 'texts/paul_graham_essay.txt'}),\n",
              " Document(page_content=\"One of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm writing, and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious intellectually in something like the way Latin is, also seems about as hip.\", metadata={'source': 'texts/paul_graham_essay.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "retriever.get_relevant_documents(\n",
        "    \"What did the author liked looking at?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT7U1MPhOjFq"
      },
      "source": [
        "## What now?\n",
        "\n",
        "This demo is hosted [here](https://cassio.org/frameworks/langchain/qa-basic/) at cassio.org.\n",
        "\n",
        "Discover the other ways you can integrate\n",
        "Cassandra/Astra DB with your ML/GenAI needs,\n",
        "right **within [your favorite framework](https://cassio.org/frameworks/langchain/about/)**."
      ],
      "id": "OT7U1MPhOjFq"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}